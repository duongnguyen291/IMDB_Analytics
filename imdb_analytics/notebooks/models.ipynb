{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Định nghĩa constants\n",
    "HDFS_HOST = \"hdfs://localhost:9000\"  \n",
    "HDFS_PATH = f\"{HDFS_HOST}/hadoop/data/parquet/\"\n",
    "\n",
    "def init_spark(app_name=\"IMDb Analytics\"):\n",
    "    \"\"\"\n",
    "    Tạo và cấu hình SparkSession với các thiết lập phù hợp.\n",
    "    \n",
    "    Parameters:\n",
    "        app_name (str): Tên của ứng dụng Spark\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: SparkSession đã được cấu hình\n",
    "    \"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(\"spark://localhost:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", HDFS_HOST) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", f\"{HDFS_HOST}/user/hive/warehouse\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.driver.cores\", \"2\") \\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"64MB\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .config(\"spark.executor.instances\", \"2\")   \\\n",
    "        .getOrCreate()\n",
    "        # .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \\\n",
    "        # .config(\"spark.dynamicAllocation.maxExecutors\", \"2\") \\\n",
    "        #  .config(\"spark.dynamicAllocation.enabled\", \"true\") \n",
    "       \n",
    "\n",
    "# Hàm tiện ích để kiểm tra kết nối HDFS\n",
    "def test_hdfs_connection(spark):\n",
    "    \"\"\"\n",
    "    Kiểm tra kết nối tới HDFS bằng cách đọc thử một file parquet\n",
    "    \n",
    "    Parameters:\n",
    "        spark (SparkSession): SparkSession đã được khởi tạo\n",
    "        \n",
    "    Returns:\n",
    "        bool: True nếu kết nối thành công, False nếu thất bại\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Thử đọc một file parquet bất kỳ\n",
    "        test_df = spark.read.parquet(f\"{HDFS_PATH}/title_basics_parquet\")\n",
    "        test_df.printSchema()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi kết nối HDFS: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataLoader:\n",
    "    def __init__(self, spark, base_path):\n",
    "        self.spark = spark\n",
    "        self.base_path = base_path\n",
    "    \n",
    "    def load_titles(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_basics_parquet\") # basic in4 about titles\n",
    "    \n",
    "    def load_ratings(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_ratings_parquet\") # in4 about ratings and vote counts for titles\n",
    "    \n",
    "    def load_names(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/name_basics_parquet\") # Basic in4 about individuals\n",
    "\n",
    "    def load_akas(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_akas_parquet\") # In4 about alternative titles of movies or shows\n",
    "        \n",
    "    def load_episodes(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_episode_parquet\") # About episodoes in a series\n",
    "\n",
    "    def load_principals(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_principals_parquet\") # In4 about key indivisuals related to a title\n",
    "    \n",
    "    def load_crews(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_crew_parquet\") # In4 about the creative team behind the film\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng workers đang hoạt động: 4\n",
      "Executor Info: org.apache.spark.SparkExecutorInfoImpl@1c4bf79\n",
      "Executor Info: org.apache.spark.SparkExecutorInfoImpl@6b719de5\n",
      "Executor Info: org.apache.spark.SparkExecutorInfoImpl@d247f24\n",
      "Executor Info: org.apache.spark.SparkExecutorInfoImpl@332e001e\n",
      "Executor Info: org.apache.spark.SparkExecutorInfoImpl@17354e75\n"
     ]
    }
   ],
   "source": [
    "# When have 2 workers\n",
    "spark = init_spark()\n",
    "\n",
    "loader = IMDbDataLoader(spark, \"hdfs:///hadoop/data/parquet/\")\n",
    "\n",
    "# Sử dụng SparkContext để lấy thông tin về các executors\n",
    "workers_info = spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos()\n",
    "\n",
    "# Đếm số lượng workers (executors)\n",
    "num_workers = len(workers_info) - 1\n",
    "\n",
    "# In ra số lượng workers và thông tin chi tiết\n",
    "print(f\"Số lượng workers đang hoạt động: {num_workers}\")\n",
    "for worker in workers_info:\n",
    "    print(f\"Executor Info: {worker}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:==============>                                           (1 + 1) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:==============>                                           (1 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "titles_df = loader.load_titles()\n",
    "rating_df = loader.load_ratings()\n",
    "names_df = loader.load_names()\n",
    "akas_df = loader.load_akas()\n",
    "episode_df = loader.load_episodes()\n",
    "principals_df = loader.load_principals()\n",
    "crew_df = loader.load_crews()\n",
    "principal_df = loader.load_principals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/15 23:57:42 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/15 23:58:25 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/15 23:59:08 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/15 23:59:18 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/15 23:59:38 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/15 23:59:46 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:00:03 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:00:16 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:00:40 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:00:59 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:01:33 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:01:56 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:02:14 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:02:31 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:02:49 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:03:04 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:03:22 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:03:40 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:03:58 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:04:16 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:04:31 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:04:48 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:05:05 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:05:20 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:05:37 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:05:51 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:06:08 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:06:24 WARN DAGScheduler: Broadcasting large task binary with size 59.5 MiB\n",
      "25/01/16 00:09:07 WARN TaskSetManager: Lost task 3.0 in stage 78.0 (TID 304) (192.168.80.128 executor 3): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$754/1730099790.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n",
      "\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda$767/829410647.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$766/1849362889.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$764/443586943.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n",
      "\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n",
      "\n",
      "25/01/16 00:09:17 ERROR TaskSchedulerImpl: Lost executor 3 on 192.168.80.128: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/16 00:09:22 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.80.128: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_6 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_3 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_4 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_1 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_8 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_0 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_6 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_3 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_8 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_5 !\n",
      "25/01/16 00:09:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_9 !\n",
      "25/01/16 00:09:28 WARN TaskSetManager: Lost task 1.0 in stage 78.0 (TID 302) (192.168.80.128 executor 0): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$754/556083243.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n",
      "\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda$767/585759189.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$766/1026408929.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$764/1649563972.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n",
      "\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n",
      "\n",
      "25/01/16 00:09:49 ERROR TransportClient: Failed to send RPC RPC 6575158828052340279 to /192.168.80.128:45256: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/01/16 00:10:09 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.80.128: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/16 00:09:49 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 66 from block manager BlockManagerId(0, 192.168.80.128, 39677, None)\n",
      "java.io.IOException: Failed to send RPC RPC 6575158828052340279 to /192.168.80.128:45256: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:966)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:934)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:943)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:966)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:934)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:943)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/01/16 00:10:09 WARN TaskSetManager: Lost task 3.2 in stage 78.0 (TID 307) (192.168.80.128 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_66_piece5 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_66_piece4 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_4 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_7 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_283_2 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_66_piece0 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_66_piece1 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_0 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_66_piece2 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_66_piece3 !\n",
      "25/01/16 00:10:09 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_66_piece6 !\n",
      "25/01/16 00:10:10 ERROR Utils: Uncaught exception in thread task-result-getter-2\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1$adapted(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$4108/536805837.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1486)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1105)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1458)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1083)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$2(ChunkedByteBuffer.scala:245)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$4109/1411085523.apply$mcI$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:246)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromManagedBuffer(ChunkedByteBuffer.scala:220)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBytes$1(BlockManager.scala:1263)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2827/1502611068.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$5(BlockManager.scala:1105)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4107/1926917780.apply(Unknown Source)\n",
      "\tat scala.Option.flatMap(Option.scala:271)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$4(BlockManager.scala:1103)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2831/643850463.apply(Unknown Source)\n",
      "\tat scala.Option.flatMap(Option.scala:271)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1100)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3$$Lambda$1680/687562865.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "Exception in thread \"task-result-getter-2\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1$adapted(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$4108/536805837.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1486)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1105)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1458)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1083)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$2(ChunkedByteBuffer.scala:245)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$4109/1411085523.apply$mcI$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:246)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromManagedBuffer(ChunkedByteBuffer.scala:220)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBytes$1(BlockManager.scala:1263)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2827/1502611068.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$5(BlockManager.scala:1105)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4107/1926917780.apply(Unknown Source)\n",
      "\tat scala.Option.flatMap(Option.scala:271)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$4(BlockManager.scala:1103)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2831/643850463.apply(Unknown Source)\n",
      "\tat scala.Option.flatMap(Option.scala:271)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1100)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3$$Lambda$1680/687562865.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "25/01/16 00:10:26 WARN BlockManager: Failed to fetch remote block taskresult_301 from [BlockManagerId(1, 192.168.80.128, 33839, None)] after 1 fetch failures. Most recent failure cause:\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1172)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.ExecutionException: Boxed Error\n",
      "\tat scala.concurrent.impl.Promise$.resolver(Promise.scala:87)\n",
      "\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:79)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.failure(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.failure$(Promise.scala:104)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:187)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:98)\n",
      "\tat org.apache.spark.network.shuffle.BlockFetchingListener.onBlockTransferSuccess(BlockFetchingListener.java:37)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockTransferSuccess(BlockTransferService.scala:81)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.handleBlockTransferSuccess(RetryingBlockTransferor.java:266)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.onBlockFetchSuccess(RetryingBlockTransferor.java:302)\n",
      "\tat org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:286)\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:172)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat io.netty.buffer.CompositeByteBuf.nioBuffer(CompositeByteBuf.java:1685)\n",
      "\tat io.netty.buffer.AbstractDerivedByteBuf.nioBuffer(AbstractDerivedByteBuf.java:122)\n",
      "\tat io.netty.buffer.AbstractByteBuf.nioBuffer(AbstractByteBuf.java:1231)\n",
      "\tat org.apache.spark.network.buffer.NettyManagedBuffer.nioByteBuffer(NettyManagedBuffer.java:46)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:94)\n",
      "\tat org.apache.spark.network.shuffle.BlockFetchingListener.onBlockTransferSuccess(BlockFetchingListener.java:37)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockTransferSuccess(BlockTransferService.scala:81)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.handleBlockTransferSuccess(RetryingBlockTransferor.java:266)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.onBlockFetchSuccess(RetryingBlockTransferor.java:302)\n",
      "\tat org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:286)\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:172)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "25/01/16 00:10:26 WARN TaskSetManager: Lost task 0.0 in stage 78.0 (TID 301) (192.168.80.128 executor 1): TaskResultLost (result lost from block manager)\n",
      "25/01/16 00:10:33 WARN BlockManager: Failed to fetch remote block taskresult_309 from [BlockManagerId(1, 192.168.80.128, 33839, None)] after 1 fetch failures. Most recent failure cause:\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1172)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.ExecutionException: Boxed Error\n",
      "\tat scala.concurrent.impl.Promise$.resolver(Promise.scala:87)\n",
      "\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:79)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.failure(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.failure$(Promise.scala:104)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:187)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:98)\n",
      "\tat org.apache.spark.network.shuffle.BlockFetchingListener.onBlockTransferSuccess(BlockFetchingListener.java:37)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockTransferSuccess(BlockTransferService.scala:81)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.handleBlockTransferSuccess(RetryingBlockTransferor.java:266)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.onBlockFetchSuccess(RetryingBlockTransferor.java:302)\n",
      "\tat org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:286)\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:172)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/16 00:10:33 WARN TaskSetManager: Lost task 3.3 in stage 78.0 (TID 309) (192.168.80.128 executor 1): TaskResultLost (result lost from block manager)\n",
      "25/01/16 00:10:42 WARN BlockManager: Failed to fetch remote block taskresult_310 from [BlockManagerId(1, 192.168.80.128, 33839, None)] after 1 fetch failures. Most recent failure cause:\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1172)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.ExecutionException: Boxed Error\n",
      "\tat scala.concurrent.impl.Promise$.resolver(Promise.scala:87)\n",
      "\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:79)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.failure(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.failure$(Promise.scala:104)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:187)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:98)\n",
      "\tat org.apache.spark.network.shuffle.BlockFetchingListener.onBlockTransferSuccess(BlockFetchingListener.java:37)\n",
      "\tat org.apache.spark.network.BlockTransferService$$anon$1.onBlockTransferSuccess(BlockTransferService.scala:81)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.handleBlockTransferSuccess(RetryingBlockTransferor.java:266)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.onBlockFetchSuccess(RetryingBlockTransferor.java:302)\n",
      "\tat org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:286)\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:172)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "25/01/16 00:10:43 WARN TaskSetManager: Lost task 0.1 in stage 78.0 (TID 310) (192.168.80.128 executor 1): TaskResultLost (result lost from block manager)\n",
      "25/01/16 00:10:49 ERROR TaskSetManager: Total size of serialized results of 6 tasks (1139.8 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n",
      "25/01/16 00:10:49 WARN TaskSetManager: Lost task 3.4 in stage 78.0 (TID 311) (192.168.80.128 executor 1): TaskKilled (Tasks result size has exceeded maxResultSize)\n",
      "25/01/16 00:11:49 WARN TaskSetManager: Lost task 0.2 in stage 78.0 (TID 312) (192.168.80.128 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Total size of serialized results of 6 tasks (1139.8 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB))\n",
      "[Stage 78:==============>                                           (1 + 1) / 4]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o304.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 6 tasks (1139.8 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4160)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 205\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_recommendations(\n\u001b[1;32m    196\u001b[0m             movie_id,\n\u001b[1;32m    197\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_matrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m             alpha\n\u001b[1;32m    203\u001b[0m         )\n\u001b[1;32m    204\u001b[0m recommender \u001b[38;5;241m=\u001b[39m MovieRecommender()\n\u001b[0;32m--> 205\u001b[0m \u001b[43mrecommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Lấy đề xuất cho một bộ phim\u001b[39;00m\n\u001b[1;32m    208\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m recommender\u001b[38;5;241m.\u001b[39mrecommend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtt0111161\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Ví dụ với ID của phim The Shawshank Redemption\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 192\u001b[0m, in \u001b[0;36mMovieRecommender.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Train ALS model\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mals_model,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexer \u001b[38;5;241m=\u001b[39m train_als_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_data)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_matrix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_pd \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_similarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_data_with_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mcompute_similarity_matrix\u001b[0;34m(movie_data_with_embeddings)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_similarity_matrix\u001b[39m(movie_data_with_embeddings):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Convert to pandas for easier similarity computation\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     embeddings_pd \u001b[38;5;241m=\u001b[39m \u001b[43mmovie_data_with_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtconst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprimaryTitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenre_embedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m---> 82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     embeddings_pd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m embeddings_pd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mtoArray())\n\u001b[1;32m     86\u001b[0m     n_movies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(embeddings_pd)\n",
      "File \u001b[0;32m~/GR1/IMDB/IMDB_Analytics/venv/lib/python3.12/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/GR1/IMDB/IMDB_Analytics/venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/GR1/IMDB/IMDB_Analytics/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/GR1/IMDB/IMDB_Analytics/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/GR1/IMDB/IMDB_Analytics/venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o304.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 6 tasks (1139.8 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4160)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:==============>                                           (1 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec, VectorAssembler, Normalizer, StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import split, col, udf, lit\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(titles_df,rating_df):\n",
    "    \n",
    "    # Split genres into array\n",
    "    titles_df = titles_df.fillna({'genres': ''})\n",
    "    \n",
    "    titles_df = titles_df.withColumn(\n",
    "        \"genres_array\",\n",
    "        split(col(\"genres\"), \",\")\n",
    "    )\n",
    "    titles_df = titles_df.filter(\n",
    "    (col(\"genres_array\").isNotNull()) & \n",
    "    (col(\"genres_array\").getItem(0) != \"\")\n",
    "    )   \n",
    "    # Join with ratings\n",
    "    movie_data = titles_df.join(rating_df, \"tconst\", \"inner\")\n",
    "    \n",
    "    return movie_data\n",
    "\n",
    "def create_genre_embeddings(movie_data):\n",
    "    # Configure Word2Vec model\n",
    "    word2vec = Word2Vec(\n",
    "        vectorSize=100,\n",
    "        minCount=1,\n",
    "        inputCol=\"genres_array\",\n",
    "        outputCol=\"genre_embedding\"\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    model = word2vec.fit(movie_data)\n",
    "    movie_data_with_embeddings = model.transform(movie_data)\n",
    "    \n",
    "    return movie_data_with_embeddings\n",
    "\n",
    "def train_als_model(movie_data):\n",
    "    # Convert ratings to float\n",
    "    movie_data = movie_data.withColumn(\n",
    "        \"averageRating\", \n",
    "        col(\"averageRating\").cast(\"float\")\n",
    "    )\n",
    "    # Thêm cột userId với giá trị cố định (1) cho tất cả các phim\n",
    "    movie_data = movie_data.withColumn(\"userId\", lit(1))\n",
    "    \n",
    "    # Chuyển đổi 'tconst' từ string sang numeric\n",
    "    indexer = StringIndexer(inputCol=\"tconst\", outputCol=\"tconst_index\")\n",
    "    movie_data = indexer.fit(movie_data).transform(movie_data)\n",
    "    # Initialize ALS model\n",
    "    als = ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"tconst_index\",\n",
    "        ratingCol=\"averageRating\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True,\n",
    "        rank=10,\n",
    "        maxIter=10\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    model = als.fit(movie_data)\n",
    "    \n",
    "    return model , indexer\n",
    "\n",
    "def compute_similarity_matrix(movie_data_with_embeddings):\n",
    "\n",
    "    limited_data = movie_data_with_embeddings.limit(5000)\n",
    "\n",
    "    # Convert to pandas for easier similarity computation\n",
    "    embeddings_pd = limited_data.select(\n",
    "        \"tconst\", \n",
    "        \"primaryTitle\", \n",
    "        \"genre_embedding\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    embeddings_pd['genre_embedding'] = embeddings_pd['genre_embedding'].apply(lambda x: x.toArray())\n",
    "    \n",
    "    n_movies = len(embeddings_pd)\n",
    "    similarity_matrix = np.zeros((n_movies, n_movies))\n",
    "    \n",
    "    # Compute cosine similarity between all pairs\n",
    "    for i in range(n_movies):\n",
    "        for j in range(i+1, n_movies):\n",
    "            sim = 1 - cosine(\n",
    "                embeddings_pd.iloc[i][\"genre_embedding\"],\n",
    "                embeddings_pd.iloc[j][\"genre_embedding\"]\n",
    "            )\n",
    "            similarity_matrix[i,j] = sim\n",
    "            similarity_matrix[j,i] = sim\n",
    "    \n",
    "    return similarity_matrix, embeddings_pd\n",
    "\n",
    "def get_recommendations(movie_id, similarity_matrix, embeddings_pd, als_model, indexer, spark, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Lấy đề xuất phim dựa trên nội dung và collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "        movie_id (str): ID của phim muốn lấy đề xuất\n",
    "        similarity_matrix (ndarray): Ma trận tương đồng giữa các phim\n",
    "        embeddings_pd (DataFrame): DataFrame pandas chứa thông tin phim và embeddings\n",
    "        als_model (ALSModel): Mô hình ALS đã được huấn luyện\n",
    "        indexer (StringIndexerModel): Mô hình StringIndexer đã được huấn luyện\n",
    "        spark (SparkSession): SparkSession hiện tại\n",
    "        alpha (float): Hệ số cân bằng giữa content-based và collaborative filtering\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame pandas: Các phim được đề xuất\n",
    "    \"\"\"\n",
    "    # Lấy index của phim dựa trên 'tconst'\n",
    "    tconst_index_row = indexer.transform(spark.createDataFrame([(movie_id,)], [\"tconst\"]))\n",
    "    tconst_index = tconst_index_row.select(\"tconst_index\").first()[\"tconst_index\"]\n",
    "    \n",
    "    # Get content-based recommendations\n",
    "    movie_idx = embeddings_pd[embeddings_pd[\"tconst\"] == movie_id].index[0]\n",
    "    content_scores = similarity_matrix[movie_idx]\n",
    "    \n",
    "    # Get collaborative filtering recommendations\n",
    "    cf_predictions = als_model.recommendForItemSubset(\n",
    "        spark.createDataFrame([(tconst_index,)], [\"tconst_index\"]), \n",
    "        100\n",
    "    )\n",
    "    cf_scores = cf_predictions.toPandas()\n",
    "    \n",
    "    # Chuyển đổi cột 'recommendations' từ Struct thành list\n",
    "    if 'recommendations' in cf_scores.columns:\n",
    "        cf_scores = cf_scores.explode('recommendations')\n",
    "        cf_scores['tconst_index_cf'] = cf_scores['recommendations'].apply(lambda x: x['tconst_index'])\n",
    "        cf_scores['rating_cf'] = cf_scores['recommendations'].apply(lambda x: x['rating'])\n",
    "    else:\n",
    "        cf_scores['tconst_index_cf'] = cf_scores['tconst_index']\n",
    "        cf_scores['rating_cf'] = 0\n",
    "    \n",
    "    # Kết hợp điểm số\n",
    "    final_scores = {}\n",
    "    for idx, row in embeddings_pd.iterrows():\n",
    "        movie = row[\"tconst\"]\n",
    "        content_score = content_scores[idx]\n",
    "        \n",
    "        # Lấy điểm từ collaborative filtering\n",
    "        # Chuyển 'tconst' sang 'tconst_index'\n",
    "        movie_index_row = indexer.transform(spark.createDataFrame([(movie,)], [\"tconst\"]))\n",
    "        movie_index = movie_index_row.select(\"tconst_index\").first()[\"tconst_index\"]\n",
    "        \n",
    "        cf_row = cf_scores[cf_scores[\"tconst_index_cf\"] == movie_index]\n",
    "        cf_score = cf_row[\"rating_cf\"].values[0] if not cf_row.empty else 0\n",
    "        \n",
    "        final_scores[movie] = alpha * content_score + (1 - alpha) * cf_score\n",
    "    \n",
    "    # Sắp xếp và lấy top 10 đề xuất\n",
    "    recommendations = sorted(\n",
    "        final_scores.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )[:10]\n",
    "    \n",
    "    # Lấy thông tin phim chi tiết\n",
    "    recommended_movies = embeddings_pd[\n",
    "        embeddings_pd[\"tconst\"].isin([r[0] for r in recommendations])\n",
    "    ][[\"tconst\", \"primaryTitle\"]]\n",
    "    \n",
    "    return recommended_movies\n",
    "\n",
    "class MovieRecommender:\n",
    "    def __init__(self):\n",
    "        self.spark = init_spark()\n",
    "        self.movie_data = None\n",
    "        self.als_model = None\n",
    "        self.similarity_matrix = None\n",
    "        self.embeddings_pd = None\n",
    "        self.indexer = None\n",
    "    \n",
    "    def fit(self):\n",
    "        # Load and preprocess data\n",
    "        titles_df = loader.load_titles()\n",
    "        rating_df = loader.load_ratings()\n",
    "        self.movie_data = load_and_preprocess_data(titles_df,rating_df)\n",
    "        \n",
    "        # Create genre embeddings\n",
    "        movie_data_with_embeddings = create_genre_embeddings(self.movie_data)\n",
    "        \n",
    "        # Train ALS model\n",
    "        self.als_model,self.indexer = train_als_model(self.movie_data)\n",
    "        \n",
    "        self.similarity_matrix, self.embeddings_pd = compute_similarity_matrix(movie_data_with_embeddings)\n",
    "    \n",
    "    def recommend(self, movie_id, alpha=0.5):\n",
    "        return get_recommendations(\n",
    "            movie_id,\n",
    "            self.similarity_matrix,\n",
    "            self.embeddings_pd,\n",
    "            self.als_model,\n",
    "            self.indexer,\n",
    "            self.spark,\n",
    "            alpha\n",
    "        )\n",
    "recommender = MovieRecommender()\n",
    "recommender.fit()\n",
    "\n",
    "# Lấy đề xuất cho một bộ phim\n",
    "recommendations = recommender.recommend(\"tt0111161\")  # Ví dụ với ID của phim The Shawshank Redemption\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
