{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Định nghĩa constants\n",
    "HDFS_HOST = \"hdfs://localhost:9000\"  # Hoặc địa chỉ HDFS của bạn\n",
    "HDFS_PATH = f\"{HDFS_HOST}/hadoop/data/parquet/\"\n",
    "\n",
    "def create_spark_session(app_name=\"IMDb Analytics\"):\n",
    "    \"\"\"\n",
    "    Tạo và cấu hình SparkSession với các thiết lập phù hợp.\n",
    "    \n",
    "    Parameters:\n",
    "        app_name (str): Tên của ứng dụng Spark\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: SparkSession đã được cấu hình\n",
    "    \"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", HDFS_HOST) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", f\"{HDFS_HOST}/user/hive/warehouse\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.driver.cores\", \"2\") \\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Hàm tiện ích để kiểm tra kết nối HDFS\n",
    "def test_hdfs_connection(spark):\n",
    "    \"\"\"\n",
    "    Kiểm tra kết nối tới HDFS bằng cách đọc thử một file parquet\n",
    "    \n",
    "    Parameters:\n",
    "        spark (SparkSession): SparkSession đã được khởi tạo\n",
    "        \n",
    "    Returns:\n",
    "        bool: True nếu kết nối thành công, False nếu thất bại\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Thử đọc một file parquet bất kỳ\n",
    "        test_df = spark.read.parquet(f\"{HDFS_PATH}/title_basics_parquet\")\n",
    "        test_df.printSchema()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi kết nối HDFS: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Thêm thư mục gốc vào sys.path\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../..\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_spark_session\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDbDataLoader\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "class IMDbDataLoader:\n",
    "    def __init__(self, spark, base_path):\n",
    "        self.spark = spark\n",
    "        self.base_path = base_path\n",
    "    \n",
    "    def load_titles(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_basics_parquet\") # basic in4 about titles\n",
    "    \n",
    "    def load_ratings(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_ratings_parquet\") # in4 about ratings and vote counts for titles\n",
    "    \n",
    "    def load_names(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/name_basics_parquet\") # Basic in4 about individuals\n",
    "\n",
    "    def load_akas(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_akas_parquet\") # In4 about alternative titles of movies or shows\n",
    "        \n",
    "    def load_episodes(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/episode_parquet\") # About episodoes in a series\n",
    "\n",
    "    def load_principals(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_principals_parquet\") # In4 about key indivisuals related to a title\n",
    "    \n",
    "    def load_crews(self):\n",
    "        return self.spark.read.parquet(f\"{self.base_path}/title_crew_parquet\") # In4 about the creative team behind the film\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "# Đọc file Parquet\n",
    "loader = IMDbDataLoader(spark, \"hdfs:///hadoop/data/parquet/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = loader.load_titles()\n",
    "rating_df = loader.load_ratings()\n",
    "names_df = loader.load_names()\n",
    "akas_df = loader.load_akas()\n",
    "episode_df = loader.load_episodes()\n",
    "principal_df = loader.load_principals()\n",
    "crew_df = loader.load_crews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df.printSchema(5)\n",
    "titles_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
